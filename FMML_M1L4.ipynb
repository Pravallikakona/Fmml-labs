{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pravallikakona/Fmml-labs/blob/main/FMML_M1L4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU9LZjQxiQT2"
      },
      "source": [
        "# Transforming data using linear algebra\n",
        "\n",
        "FMML Module 1, Lab 4\n",
        "\n",
        "Matrix transformations are at the heart of many machine learning algorithms. In this lab, we'll visualize the effect of some simple transformations on a unit square and then visualize it using the MNIST dataset. We also see what data normalization means and how it can help in improving the accuracy of machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KBe4ZA32UTbv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7mqG-dafVpya"
      },
      "outputs": [],
      "source": [
        "# You don't need to understand these functions\n",
        "\n",
        "\n",
        "def plotGrid(transform, unit, linestyle=\":\", fig=None, ax=None):\n",
        "    lim1 = -100\n",
        "    lim2 = 100\n",
        "\n",
        "    def mat2xy(start, end):\n",
        "        if len(start.shape) == 1:\n",
        "            start = np.expand_dims(start, 0)\n",
        "            end = np.expand_dims(end, 0)\n",
        "        nan = np.ones(len(start)) * np.nan\n",
        "        x = np.stack((start[:, 0], end[:, 0], nan)).T.reshape(-1)\n",
        "        y = np.stack((start[:, 1], end[:, 1], nan)).T.reshape(-1)\n",
        "        return x, y\n",
        "\n",
        "    def parallellines(axis, addend, lines, unit):\n",
        "        addend = np.repeat(np.expand_dims(addend, 0), lines * 2, 0)\n",
        "        unit = np.expand_dims(np.arange(-lines, lines) * unit, 1)\n",
        "        unit = unit - lines\n",
        "        addend = addend * unit\n",
        "        lines = np.expand_dims(axis, 0) + addend\n",
        "        return np.concatenate((lines, lines * -1))\n",
        "\n",
        "    if fig is None:\n",
        "        fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    transform = transform.astype(float)\n",
        "    xaxis = transform[0]\n",
        "    yaxis = transform[1]\n",
        "\n",
        "    # plot lines parallel to the x axis\n",
        "    lines1 = parallellines(xaxis * lim1, yaxis, 100, unit)\n",
        "    lines2 = parallellines(xaxis * lim2, yaxis, 100, unit)\n",
        "    x, y = mat2xy(lines1, lines2)\n",
        "    plt.plot(x, y, linestyle + \"k\", linewidth=0.5)\n",
        "    # plot x axis\n",
        "    x, y = mat2xy(xaxis * lim1, xaxis * lim2)\n",
        "    plt.plot(x, y, linestyle, color=\"#440077\")\n",
        "\n",
        "    # plot  lines parallel to the y axis\n",
        "    lines1 = parallellines(yaxis * lim1, xaxis, 100, unit)\n",
        "    lines2 = parallellines(yaxis * lim2, xaxis, 100, unit)\n",
        "    x, y = mat2xy(lines1, lines2)\n",
        "    plt.plot(x, y, linestyle + \"k\", linewidth=0.5)\n",
        "    # plot y axis\n",
        "    x, y = mat2xy(yaxis * lim1, yaxis * lim2)\n",
        "    plt.plot(x, y, linestyle, color=\"#aa5500\")\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def plotData(X, y, xlabel=\"hole\", ylabel=\"bound\", fig=None, ax=None):\n",
        "    if fig is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    for ii in range(nclasses):\n",
        "        plt.scatter(X[y == ii, 0], X[y == ii, 1])\n",
        "    plt.legend([str(i) for i in range(nclasses)])\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    lim2 = X.max()\n",
        "    lim1 = X.min()\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d5dmEXxaktp"
      },
      "source": [
        "## Matrix transformations on data\n",
        "\n",
        "Note: This lab involves a lot of matrix operations. If you are not familiar with them, please go through the resources given in class before proceeding. You can also review Khan Academy's excellent linear algebra [resources](https://www.khanacademy.org/math/linear-algebra/matrix-transformations).\n",
        "\n",
        "A 2D coordinate system is defined by its basis vectors, i and j. Any point in this 2D space can be represented as a linear combination of these basis vectors. For example, the point (a,b) can be represented as:\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\} = a\\left\\{  \\begin{aligned}1 \\\\ 0 \\end{aligned} \\right\\} + b\\left\\{  \\begin{aligned}0 \\\\ 1 \\end{aligned} \\right\\} = a\\hat{i} + b\\hat{j}\n",
        "\\end{equation}$$\n",
        "\n",
        "A matrix can be used to perform a linear transformation on the basis vectors. The new basis vectors $\\hat{i}$ and $\\hat{j}$ are given by the product of the matrix and the basis vectors of the standard coordinate system.\n",
        "\n",
        "In the standard coordinate system (Let us call it T0), the basis vectors are\n",
        "\n",
        "$$\\begin{equation}\n",
        "i = \\left\\{  \\begin{aligned}1 \\\\ 0 \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "and\n",
        "$$\\begin{equation} j = \\left\\{ \\begin{aligned} 0 \\\\ 1\\end{aligned} \\right\\} \\end{equation}$$\n",
        "\n",
        "We can use any two vectors as basis vectors for a new coordinate system as long as they are not colinear. For example, let us call this new coordinate system T1:\n",
        "\n",
        "$$\\begin{equation}\n",
        "i = \\left\\{  \\begin{aligned}1 \\\\ -1 \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "and\n",
        "$$\\begin{equation} j = \\left\\{ \\begin{aligned} 0 \\\\ 2 \\end{aligned} \\right\\} \\end{equation}$$\n",
        "\n",
        "Suppose we have a point [a,b] in the T1 coordinate system. Its representation in the standard system T0 can be obtained by the following matrix multiplication:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "\\left\\{  \\begin{aligned}a' \\\\ b' \\end{aligned} \\right\\} =\n",
        "\\left\\{  \\begin{aligned}&1 & 0 \\\\ -&1 & 2 \\end{aligned} \\right\\}\n",
        "\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "where the columns of the matrix are the basis vectors of T1.\n",
        "\n",
        "\n",
        "Let us see this in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8SCFKqfbM-l",
        "outputId": "95545753-4347-42f3-d4c4-4f20a7afde4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data in T0 =  [5 3]\n",
            "Data in T1 =  [5 4]\n"
          ]
        }
      ],
      "source": [
        "T0 = np.array([[1, 0], [0, 1]])\n",
        "T1 = np.array([[1, 0], [-1, 2]])\n",
        "\n",
        "data1 = np.array([5, 4])  # the data in T1 coordinate system\n",
        "data0 = np.matmul(T1, data1)  # the data in T0 coordinate system\n",
        "\n",
        "print(\"Data in T0 = \", data0)\n",
        "print(\"Data in T1 = \", data1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIfEWZ1RQeve"
      },
      "source": [
        "We can visualize this below. T0 is shown with dotted lines and T1 is shown with solid lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "jSuTaeSDQoYK",
        "outputId": "59692c30-f41b-4b56-b715-b2cee921ba82"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plotgrid' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4e9f8220b059>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplotgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plotgrid' is not defined"
          ]
        }
      ],
      "source": [
        "fig, ax = plotgrid(T1.T, 1, \"-\")\n",
        "plotgrid(T0.T, 1, fig=fig, ax=ax)\n",
        "\n",
        "plt.scatter(data0[0], data0[1])\n",
        "ax.set_xlim(-10, 10)\n",
        "ax.set_ylim(-10, 10)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUb3oYWIgdya"
      },
      "source": [
        "Look at the coordinates of the blue dot. In T0 (dotted lines), the position is [5,3] where it is [5,4] in T1. Feel free to experiment with different data points and coordinate systems.\n",
        "\n",
        "Remember that we can achieve the same thing by post-multiplying the transpose of the transformation matrix to the data. This will come in handy when transforming multiple data points at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD4pqOMndf-4",
        "outputId": "7b094854-237a-4128-edfd-79ce5e03909e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 3]\n",
            "[5 3]\n"
          ]
        }
      ],
      "source": [
        "data0_a = np.matmul(T1, data1)\n",
        "data0_b = np.matmul(data1, T1.T)\n",
        "print(data0_a)\n",
        "print(data0_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Kui3dSESPm"
      },
      "source": [
        "Why is transforming data useful? Data transformations cause the distance between data points to change. This will affect distance-based algorithms such as nearest neighbour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE0NbpYIC9ou",
        "outputId": "f58c53fa-4559-46be-a451-239705b915e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance between A and B in T1 =  8.06225774829855\n",
            "Distance between B and C in T1 =  4.123105625617661\n",
            "Distance between A and C in T1 =  4.47213595499958\n",
            "\n",
            "Distance between A and B in T0 =  15.033296378372908\n",
            "Distance between B and C in T0 =  9.055385138137417\n",
            "Distance between A and C in T0 =  6.324555320336759\n"
          ]
        }
      ],
      "source": [
        "# let us define 3 points in T1\n",
        "A1 = np.array([3, 3])\n",
        "B1 = np.array([2, -5])\n",
        "C1 = np.array([1, -1])\n",
        "\n",
        "# the corresponding points in T0:\n",
        "A0 = np.matmul(T1, A1)\n",
        "B0 = np.matmul(T1, B1)\n",
        "C0 = np.matmul(T1, C1)\n",
        "\n",
        "\n",
        "def dist(a, b):\n",
        "    # function to calculate Euclidean distance between two points\n",
        "    diff = a - b\n",
        "    sq = diff * diff\n",
        "    return np.sqrt(sq.sum())\n",
        "\n",
        "\n",
        "# distance between the points in T1\n",
        "print(\"Distance between A and B in T1 = \", dist(A1, B1))\n",
        "print(\"Distance between B and C in T1 = \", dist(B1, C1))\n",
        "print(\"Distance between A and C in T1 = \", dist(A1, C1))\n",
        "\n",
        "print(\"\")\n",
        "# distnace between the points in T0\n",
        "print(\"Distance between A and B in T0 = \", dist(A0, B0))\n",
        "print(\"Distance between B and C in T0 = \", dist(B0, C0))\n",
        "print(\"Distance between A and C in T0 = \", dist(A0, C0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE95JMniUtDm"
      },
      "source": [
        "We see that in T1, B and C are the closest whereas in T0, A and C are the closest. These kinds of changes will affect the predictions returned by the nearest neighbour algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFbpTSBdV29C"
      },
      "source": [
        "## Transformations on MNIST\n",
        "\n",
        "Let us experiment with a subset of the MNIST dataset. We will extract two features from the database for our experiment. We will then transform the data using a transformation matrix and visualize the data in the new coordinate system. We will also see how normalization can help in improving the accuracy of the model. We will reuse previous labs code for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "22ayl0sViF5-"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel\n",
        "\n",
        "\n",
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)\n",
        "\n",
        "\n",
        "def cumArray(img):\n",
        "    img2 = img.copy()\n",
        "    for ii in range(1, img2.shape[1]):\n",
        "        # for every row, add up all the rows above it.\n",
        "        img2[ii, :] = img2[ii, :] + img2[ii - 1, :]\n",
        "    img2 = img2 > 0\n",
        "    return img2\n",
        "\n",
        "\n",
        "def getHolePixels(img):\n",
        "    \"\"\"\n",
        "    This function takes in a binary image and returns the pixels that are holes in the image\n",
        "\n",
        "    img: numpy array of shape (n,m) where n is the height of the image and m is the width of the image\n",
        "\n",
        "    returns: a binary image of the same shape as the input image where the holes are filled in\n",
        "    \"\"\"\n",
        "    im1 = cumArray(img)\n",
        "    # rotate and cumulate it again for differnt direction\n",
        "    im2 = np.rot90(cumArray(np.rot90(img)), 3)\n",
        "    im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "    im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "    # this will create a binary image with all the holes filled in.\n",
        "    hull = im1 & im2 & im3 & im4\n",
        "    # remove the original digit to leave behind the holes\n",
        "    hole = hull & ~(img > 0)\n",
        "    return hole\n",
        "\n",
        "\n",
        "def getHullPixels(img):\n",
        "    \"\"\"\n",
        "    This function takes in a binary image and returns the pixels that are the convex hull of the image\n",
        "\n",
        "    img: numpy array of shape (n,m) where n is the height of the image and m is the width of the image\n",
        "\n",
        "    returns: a binary image of the same shape as the input image where the convex hull is filled in\n",
        "    \"\"\"\n",
        "    im1 = cumArray(img)\n",
        "    # rotate and cumulate it again for differnt direction\n",
        "    im2 = np.rot90(cumArray(np.rot90(img)), 3)\n",
        "    im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "    im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "    # this will create a binary image with all the holes filled in.\n",
        "    hull = im1 & im2 & im3 & im4\n",
        "    return hull\n",
        "\n",
        "\n",
        "def minus(a, b):\n",
        "    \"\"\"\n",
        "    This function takes in two binary images and returns the difference between the two images\n",
        "    \"\"\"\n",
        "    return a & ~b\n",
        "\n",
        "\n",
        "def getBoundaryPixels(img):\n",
        "    \"\"\"\n",
        "    This function takes in a binary image and returns the pixels that are the boundary of the image\n",
        "\n",
        "    img: numpy array of shape (n,m) where n is the height of the image and m is the width of the image\n",
        "\n",
        "    returns: a binary image of the same shape as the input image where the boundary is filled in\n",
        "    \"\"\"\n",
        "    img = img.copy() > 0  # binarize the image\n",
        "    rshift = np.roll(img, 1, 1)\n",
        "    lshift = np.roll(img, -1, 1)\n",
        "    ushift = np.roll(img, -1, 0)\n",
        "    dshift = np.roll(img, 1, 0)\n",
        "    boundary = (\n",
        "        minus(img, rshift)\n",
        "        | minus(img, lshift)\n",
        "        | minus(img, ushift)\n",
        "        | minus(img, dshift)\n",
        "    )\n",
        "    return boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHz5BVmLUjzb",
        "outputId": "b17f5054-83fc-495e-be6f-590cc51d570a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X / 255\n",
        "test_X = test_X / 255\n",
        "\n",
        "nclasses = 4\n",
        "\n",
        "# get only for the first 4 classes\n",
        "train_X = train_X[train_y < nclasses]\n",
        "train_y = train_y[train_y < nclasses]\n",
        "test_X = test_X[test_y < nclasses]\n",
        "test_y = test_y[test_y < nclasses]\n",
        "\n",
        "# We are only taking a subset of the training set\n",
        "train_X = train_X[::100].copy()\n",
        "train_y = train_y[::100].copy()  # do the same to the labels\n",
        "\n",
        "# taking a subset of the test set. This code takes every 500th sample\n",
        "test_X = test_X[::100].copy()\n",
        "test_y = test_y[::100].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvavjmfvH515"
      },
      "outputs": [],
      "source": [
        "# feature extraction\n",
        "train_hole = np.array([getHolePixels(i).sum() for i in train_X])\n",
        "test_hole = np.array([getHolePixels(i).sum() for i in test_X])\n",
        "train_bound = np.array([getBoundaryPixels(i).sum() for i in train_X])\n",
        "test_bound = np.array([getBoundaryPixels(i).sum() for i in test_X])\n",
        "# train_hull = np.array([getHullPixels(i).sum() for i in train_X])\n",
        "# test_hull = np.array([getHullPixels(i).sum() for i in test_X])\n",
        "# train_sum = np.sum(train_X, (1, 2)) / (28 * 28)\n",
        "# test_sum = np.sum(test_X, (1, 2)) / (28 * 28)\n",
        "\n",
        "# create the train and test set by combining the appropriate features\n",
        "train_feats = np.vstack(\n",
        "    (train_hole, train_bound)).transpose()\n",
        "test_feats = np.vstack(\n",
        "    (test_hole, test_bound)).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7qGVrlnQCUy"
      },
      "source": [
        "Let us plot the samples and see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "saRzHfi9QAGd",
        "outputId": "50f795c3-3a3d-43fd-b058-912bca7f83b6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plotData' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5a4e535543a1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mxlim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mylim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plotData' is not defined"
          ]
        }
      ],
      "source": [
        "# fix limits of x and y axis so that we can see what is going on\n",
        "xlim = [-100, 300]\n",
        "ylim = [-100, 300]\n",
        "fig, ax = plotData(train_feats, train_y)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCb4DikQP1ck"
      },
      "source": [
        "Check the baseline accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "rxVr6bd9PzlI",
        "outputId": "da3e2a47-86cc-4c88-8bf9-26e6c4206c67"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NN' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e9da3bba169c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"classes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NN' is not defined"
          ]
        }
      ],
      "source": [
        "test_pred = NN(train_feats, train_y, test_feats)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print(\"Baseline accuracy:\", acc*100, \"%\", \"for\", nclasses, \"classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl8Noo8pZRek"
      },
      "source": [
        "Let us try transforming the features and checking their accuracy. The intuition to using the transformation matrix is to find the basis vectors of the dataset and transform the data to a new coordinate system where the basis vectors are orthogonal. This will help in reducing the redundancy in the data and improve the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8dUPWRsEZKwo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "d3ecbd87-9e92-4d91-dec3-a67c1bc2a7a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.5 -0.5]\n",
            " [ 0.   2.5]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_feats' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-912e539453ee>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_feats_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# whatever transform we are applying to the training set should be applied to the test set also\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_feats_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_feats' is not defined"
          ]
        }
      ],
      "source": [
        "transform = np.array([[0.5, -0.5], [0, 2.5]])\n",
        "print(transform)\n",
        "\n",
        "train_feats_t = np.matmul(train_feats, transform)\n",
        "# whatever transform we are applying to the training set should be applied to the test set also\n",
        "test_feats_t = np.matmul(test_feats, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "iRH4VckHZaWv",
        "outputId": "cc3505ea-3a29-4950-fb50-25aa410b5498"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plotData' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e6dea2e643fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plotData' is not defined"
          ]
        }
      ],
      "source": [
        "fig, ax = plotData(train_feats_t, train_y)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "l9fknczfZdYF",
        "outputId": "5a6d6be8-2371-4c07-cbe9-160f94b19fbe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NN' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-556920af2019>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feats_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"classes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NN' is not defined"
          ]
        }
      ],
      "source": [
        "test_pred = NN(train_feats_t, train_y, test_feats_t)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print(\"Baseline accuracy:\", acc*100, \"%\", \"for\", nclasses, \"classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFBQlAnNZ3on"
      },
      "source": [
        "When experimenting with different transformation matrices and transformations for features in machine learning or computer vision tasks, here are key considerations for answering your questions:\n",
        "\n",
        "1. Experimenting with Transformation Matrices and Accuracy:\n",
        "\n",
        "Choice of Transform: The accuracy of a model after applying a transformation matrix can depend heavily on how well the transformation aligns with the underlying data structure.\n",
        "\n",
        "Evaluation: You'll need to apply the transformations and then assess the model's performance (e.g., accuracy, precision, recall). The evaluation should involve:\n",
        "\n",
        "Baseline vs. Transformed: Compare the model's accuracy with and without the transformation.\n",
        "\n",
        "Cross-Validation: Ensure that the model is tested across different data splits (cross-validation) to see if the transformation generalizes well.\n",
        "\n",
        "Interpretability: You should be able to interpret how the transformation affects the features or the space they operate in (e.g., rotating, scaling, shifting, etc.).\n",
        "\n",
        "\n",
        "\n",
        "2. Will the Same Transform Work for Other Features?\n",
        "\n",
        "Consistency Across Features: Whether the same transformation will work on other features depends on the nature of the features themselves. Some key points:\n",
        "\n",
        "Linear vs. Non-Linear Features: A transformation that works for linearly separable features may not perform as well on non-linear features.\n",
        "\n",
        "Invariance: Some transformations may preserve important properties of certain features (e.g., a translation may work for spatial data like images, but not for temporal data).\n",
        "\n",
        "Dimensionality: If features have different dimensionalities or geometries, the same transformation matrix may need to be adapted or may not work at all.\n",
        "\n",
        "\n",
        "Testing on Other Features: The best approach is to test the transformation on different subsets of features and evaluate its performance. This allows you to determine whether the transformation is feature-specific or generalizable.\n",
        "\n",
        "\n",
        "Do you have specific transformation matrices or features in mind? I can guide you through applying these steps more concretely.\n",
        "\n",
        "## Questions:\n",
        "1. Experiment with different transformation matrices and check the accuracy\n",
        "2. Will the same transform used for these two features also work for other features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXCnbDsVH516"
      },
      "source": [
        "Yes, adding all four features at once might not always be the best strategy. Here are a few reasons and strategies for improving model accuracy by selecting a better combination of features:\n",
        "\n",
        "Reasons why adding all 4 features may not work:\n",
        "\n",
        "1. Feature Redundancy: Some features may be highly correlated with each other, providing redundant information. This can lead to overfitting, where the model memorizes instead of learning the underlying patterns.\n",
        "\n",
        "\n",
        "2. Irrelevant Features: Some features may not contribute to the prediction at all, or worse, they might add noise, reducing the overall accuracy of the model.\n",
        "\n",
        "\n",
        "3. Curse of Dimensionality: Adding more features increases the complexity of the model, which might not be justified by the size of the dataset. In high-dimensional space, data points become sparse, making it harder for the model to generalize.\n",
        "\n",
        "\n",
        "\n",
        "Strategies for Better Feature Combinations:\n",
        "\n",
        "1. Try 2 Features at a Time: Adding only 2 features at a time can help in identifying whether certain pairs of features have higher predictive power compared to using all 4 at once. This process of selecting feature subsets is called feature selection.\n",
        "\n",
        "Test each pair of features by training the model and evaluating the accuracy.\n",
        "\n",
        "Once the best pair of features is identified, try to gradually add one more feature to see if the performance improves further.\n",
        "\n",
        "\n",
        "\n",
        "2. Use Feature Selection Techniques:\n",
        "\n",
        "Correlation Analysis: Check the correlation matrix to identify features that are highly correlated with each other. Removing or combining these features might reduce redundancy.\n",
        "\n",
        "Recursive Feature Elimination (RFE): This technique recursively removes the least important features and builds the model again to find the best subset of features.\n",
        "\n",
        "Principal Component Analysis (PCA): If dimensionality reduction is needed, PCA can be used to transform features into a smaller set of uncorrelated components, while retaining most of the variance in the data.\n",
        "\n",
        "\n",
        "\n",
        "3. Feature Importance:\n",
        "\n",
        "If you’re using tree-based models (e.g., random forests or gradient boosting), these models have built-in mechanisms to rank feature importance. You can then choose the most important features instead of using all four.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "How to Test 2 Features at a Time:\n",
        "\n",
        "Step 1: Select different pairs of features (e.g., Feature 1 and Feature 2, Feature 1 and Feature 3, etc.).\n",
        "\n",
        "Step 2: Train the model on each combination and evaluate its accuracy on a validation set or via cross-validation.\n",
        "\n",
        "Step 3: Compare the accuracies of different combinations and identify the pairs that perform best.\n",
        "\n",
        "\n",
        "Example Approach:\n",
        "\n",
        "1. Train a model with Feature 1 and Feature 2.\n",
        "\n",
        "\n",
        "2. Train a model with Feature 1 and Feature 3.\n",
        "\n",
        "\n",
        "3. Train a model with Feature 1 and Feature 4.\n",
        "\n",
        "\n",
        "4. Continue with different pairs, then compare their accuracies.\n",
        "\n",
        "\n",
        "\n",
        "Once you've tested various pairs, you can try adding a third or fourth feature incrementally to see if the accuracy improves without introducing noise or overfitting.\n",
        "\n",
        "Would you like to explore a specific feature set, or are you using a specific model type for which we could test different feature combinations?\n",
        "\n",
        "> Exercise: Is it possible that adding all 4 features at a time is not the best strategy? Can you think of a better combination of features that can help in improving the accuracy of the model? Maybe you can try adding 2 features at a time and see if that helps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36TOA47xak20"
      },
      "source": [
        "# Data normalization\n",
        "\n",
        "Sometimes the features of our data have vastly different scales. This will cause the learning algorithm to give more importance to certain features, reducing its performance. Data normalization is a method in which we transform the features so that they have similar scales.\n",
        "\n",
        "Three commonly used feature scaling techniques are rescaling, mean normalization and z-score normalization. Here, we will talk about the simplest one: rescaling.\n",
        "\n",
        "$$\\begin{equation}\n",
        "x' = \\frac {x -min(x)} { max(x) - min(x)}\n",
        "\\end{equation}$$\n",
        "\n",
        "\n",
        "\n",
        "For more information, see [this page](https://towardsdatascience.com/data-normalization-in-machine-learning-395fdec69d02)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni19QKDLZzeo"
      },
      "outputs": [],
      "source": [
        "def rescale(data):\n",
        "    return (data - data.min()) / (data.max() - data.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k83ZMtKeZrQ"
      },
      "source": [
        "We have to apply the rescaling to each feature individually. Also remember to apply the same transform we are using on the train set to the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JLOPwhvehpR"
      },
      "outputs": [],
      "source": [
        "train_feats_rescaled_x = rescale(train_feats[:, 0])\n",
        "train_feats_rescaled_y = rescale(train_feats[:, 1])\n",
        "train_feats_rescaled = np.stack((train_feats_rescaled_x, train_feats_rescaled_y), 1)\n",
        "\n",
        "test_feats_rescaled_x = rescale(test_feats[:, 0])\n",
        "test_feats_rescaled_y = rescale(test_feats[:, 1])\n",
        "test_feats_rescaled = np.stack((test_feats_rescaled_x, test_feats_rescaled_y), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaZVy9vxfKwX"
      },
      "source": [
        "Let us plot the rescaled features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "LXyatpwZevOH",
        "outputId": "f043fab3-ede3-4aef-9f0c-3941d2791142"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plotData' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-eb755dee695e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats_rescaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plotData' is not defined"
          ]
        }
      ],
      "source": [
        "fig, ax = plotData(train_feats_rescaled, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBgxE6zglsL"
      },
      "source": [
        "This type of rescaling makes all the features between 0 and 1.\n",
        "\n",
        "Let us calculate the accuracy obtained by this transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "hKQnsj-KgNZc",
        "outputId": "88e3c47a-9660-43a7-cd88-2a8a34d4374e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NN' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1846f6242484>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats_rescaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feats_rescaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy after transform:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NN' is not defined"
          ]
        }
      ],
      "source": [
        "test_pred = NN(train_feats_rescaled, train_y, test_feats_rescaled)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print(\"Accuracy after transform:\", acc*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qYX90Gqg-jO"
      },
      "source": [
        "All 2D linear transformations can be repreented by a transformation matrix. So what is the matrix associated with the rescaling function? Actually, we cannot represent rescaling with a matrix multiplication, because it is not a linear transform. Rescaling involves shifting the origin of the data, which is not allowed under linear transformations.\n",
        "\n",
        "We can represent rescaling as a matrix multiplication followed by a vector addition. Let our first feature vector be called X and second feature vector be called Y. Suppose we want to rescale a data point [a,b]\n",
        "\n",
        "$$ \\begin{equation}\n",
        " \\left\\{  \\begin{aligned}a' \\\\ b' \\end{aligned} \\right\\} =\n",
        " \\left\\{  \\begin{aligned} \\frac{a - min(X)}{max(X) - min(X)} \\\\ \\frac{b - min(Y)}{max(Y) - min(Y)} \\end{aligned} \\right\\} =\n",
        " \\left\\{  \\begin{aligned}&\\frac{1}{max(X)-min(X)} &0\\\\ &0 &\\frac{1}{max(Y)-min(Y)} \\end{aligned}\n",
        " \\right\\}\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\} +\n",
        " \\left\\{  \\begin{aligned} \\frac{ -min(X)}{max(X) - min(X)} \\\\ \\frac{-min(Y)}{max(Y) - min(Y)} \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "\n",
        "You can verify this yourself if you wish, though it is not necessary.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}